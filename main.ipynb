{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api import openai_api_key\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "# os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "tool = YouTubeSearchTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_id(url: str) -> str:\n",
    "    \"\"\"Extract video ID from YouTube URL\"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        if parsed_url.hostname in ['www.youtube.com', 'youtube.com']:\n",
    "            return parse_qs(parsed_url.query)['v'][0]\n",
    "        elif parsed_url.hostname == 'youtu.be':\n",
    "            return parsed_url.path[1:]\n",
    "    except:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# urls_string = tool.run(\"lex friedman, 5\")\n",
    "# # A tool to make string into a list of urls\n",
    "# urls = ast.literal_eval(urls_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# A tool to get the title of a youtube video. It is much faster that \"yt_dpl\"\n",
    "def get_youtube_title(url):\n",
    "    try:\n",
    "        # Get the page content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find title meta tag\n",
    "        title = soup.find('meta', property='og:title')\n",
    "        if title:\n",
    "            return title['content']\n",
    "            \n",
    "        # Alternative method: find title tag\n",
    "        title = soup.find('title')\n",
    "        if title:\n",
    "            # Clean up the title (remove \"- YouTube\" suffix)\n",
    "            return re.sub(r'\\s*-\\s*YouTube$', '', title.string)\n",
    "            \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting title from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "title = get_youtube_title(url=\"https://youtu.be/cY-0TRj-teI?si=mEpB7q-tXZtroHnr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_content(url: str) -> tuple:\n",
    "    \"\"\"Safely get video content and metadata\"\"\"\n",
    "    try:\n",
    "        loader = YoutubeLoader.from_youtube_url(\n",
    "            url,\n",
    "            add_video_info=False,\n",
    "            language=['en', 'ko']  # Support both English and Korean\n",
    "        )\n",
    "        \n",
    "        content = loader.load()\n",
    "        \n",
    "        if not content or len(content) == 0:\n",
    "            return None, None\n",
    "            \n",
    "        return content[0].page_content, content[0].metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading content for {url}: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct a Dictionary of titles as a key of the dictionary and values as \"content\", \"metadata\" and \"url\"\n",
    "def get_dict(urls):\n",
    "    video_dict = {}\n",
    "    for url in urls:\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        title = get_youtube_title(url)\n",
    "        content, metadata = get_video_content(url)\n",
    "\n",
    "        if content is None:\n",
    "            print(f\"Skipping {url} due to content loading error\")\n",
    "            continue\n",
    "\n",
    "        video_dict[title] = {\"content\": content,\n",
    "                        \"metadata\": metadata,\n",
    "                        \"url\": url}  \n",
    "    return video_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(title, content):\n",
    "    # Given title and the content, return the similarity of the title and content in percentage\n",
    "    llm = OpenAI(temperature=0)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    title_embed = openai_embed.embed_query(title)\n",
    "    \n",
    "    content_embed = openai_embed.embed_query(content)\n",
    "    similarity = cosine_similarity(title_embed, content_embed)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fact_checker(query, num_videos=5):\n",
    "    similarity_list = []\n",
    "    tool = YouTubeSearchTool()\n",
    "    string_urls = tool.run(f\"{query}, {num_videos}\")\n",
    "    urls = ast.literal_eval(string_urls) # Now we have the list of urls\n",
    "\n",
    "    video_dict = get_dict(urls)\n",
    "    similarity_key_dict = {}\n",
    "\n",
    "    for key in video_dict.keys():\n",
    "        title=key\n",
    "        content = video_dict[key][\"content\"]\n",
    "        url = video_dict[key][\"url\"]\n",
    "        similarity = check_similarity(title=title, content=content)\n",
    "        similarity = round(similarity*100, 2)\n",
    "        similarity_list.append(similarity)\n",
    "        video_dict[key][\"similarity\"] = str(similarity)\n",
    "\n",
    "        similarity_key_dict[str(similarity)] = {\"title\": title,\n",
    "                                                \"content\": content,\n",
    "                                                \"url\": url}\n",
    "\n",
    "    return video_dict, similarity_list, similarity_key_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dict, similarity_list, similarity_key_dict = fact_checker(\"í•œêµ­ ëŒ€í†µë ¹ íƒ„í•µ \", num_videos=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88.31, 86.31, 85.12, 83.15, 85.08, 89.37, 87.8, 87.36, 84.56, 87.02]\n"
     ]
    }
   ],
   "source": [
    "print(similarity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89.37, 88.31, 87.8, 87.36, 87.02, 86.31, 85.12, 85.08, 84.56, 83.15]\n"
     ]
    }
   ],
   "source": [
    "new_list = sorted(similarity_list,reverse=True)\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 89.37 | Title: [ì—ë””í„°í”½] ìœ¤ ëŒ€í†µë ¹ íƒ„í•µì•ˆ ê°€ê²°..ë¯¸êµ­ ì •ë¶€ê°€ ë°ížŒ ê³µì‹ ìž…ìž¥ / YTN | url: https://www.youtube.com/watch?v=4TkqFjgeAho&pp=ygUY7ZWc6rWtIOuMgO2GteuguSDtg4TtlbUg\n",
      "Similarity: 88.31 | Title: [LIVE] 'ìœ¤ì„ì—´ ëŒ€í†µë ¹ íƒ„í•µì†Œì¶”ì•ˆ' ê°€ê²°...ê´‘í™”ë¬¸ ì¼ëŒ€ 'íƒ„í•µ ë°˜ëŒ€' ì§‘íšŒ í˜„ìž¥ ìƒí™©/2024ë…„ 12ì›” 14ì¼(í† )/KBS | url: https://www.youtube.com/watch?v=vNj4lCzL4KQ&pp=ygUY7ZWc6rWtIOuMgO2GteuguSDtg4TtlbUg\n",
      "Similarity: 87.8 | Title: ë¯¸ ì–¸ë¡ , íƒ„í•µ ê°€ê²° ì¼ì œížˆ ì†ë³´ íƒ€ì „...ë¯¸ ì •ë¶€ë„ 'ì´‰ê°' / YTN | url: https://www.youtube.com/watch?v=FAxPAVmkOz0&pp=ygUY7ZWc6rWtIOuMgO2GteuguSDtg4TtlbUg\n",
      "Similarity: 87.36 | Title: 8ë…„ ì „ì²˜ëŸ¼â€¦íŠ¸ëŸ¼í”„ ì·¨ìž„ ë•Œë§ˆë‹¤ 'í•œêµ­ ëŒ€í†µë ¹ì€ íƒ„í•µ ì¤‘' / JTBC News | url: https://www.youtube.com/watch?v=cR94yJG7dUw&pp=ygUY7ZWc6rWtIOuMgO2GteuguSDtg4TtlbUg\n",
      "Similarity: 87.02 | Title: ìœ¤ì„ì—´ ëŒ€í†µë ¹ 'ë‚´ëž€' íƒ„í•µì•ˆ ê°€ê²°â€¥ì§ë¬´ì •ì§€ - [LIVE] MBC íŠ¹ì§‘ ë‰´ìŠ¤ë°ìŠ¤í¬ 2024ë…„ 12ì›” 14ì¼ | url: https://www.youtube.com/watch?v=tdr5t21hbj8&pp=ygUY7ZWc6rWtIOuMgO2GteuguSDtg4TtlbUg\n",
      "Similarity: 86.31 | Title: ì™¸ì‹ ë“¤, 'ìœ¤ íƒ„í•µ' ê¸´ê¸‰íƒ€ì „â€¦ì¤‘êµ­ í¬í„¸ ê²€ìƒ‰ì–´ 1ìœ„ì— (ìžë§‰ë‰´ìŠ¤) / SBS | url: https://www.youtube.com/watch?v=L2SO--lHJu0&pp=ygUY7ZWc6rWtIOuMgO2GteuguSDtg4TtlbUg\n",
      "Similarity: 85.12 | Title: [ìžë§‰ë‰´ìŠ¤] ì™¸ì‹ ì— 'ê·¼ì¡°í™”í™˜' ë“±ìž¥...\"í•œêµ­ì‹ ì €í•­\" K-ì‹œìœ„ì— ë¹„ìƒí•œ ê´€ì‹¬ / YTN | url: https://www.youtube.com/watch?v=cwkpwVxabmQ&pp=ygUY7ZWc6rWtIOuMgO2GteuguSDtg4TtlbUg\n",
      "Similarity: 85.08 | Title: [ðŸ”´LIVE]  ìœ¤ ëŒ€í†µë ¹ íƒ„í•µì•ˆ 'ê°€ê²°'ã…£ë‰´ìŠ¤íŠ¹ë³´ / YTN | url: https://www.youtube.com/watch?v=sQSFtdLkSB8&pp=ygUY7ZWc6rWtIOuMgO2GteuguSDtg4TtlbUg\n",
      "Similarity: 84.56 | Title: ìœ¤ì„ì—´ ëŒ€í†µë ¹, â€˜íƒ„í•µì†Œì¶”ì•ˆ ê°€ê²°â€™ ê´€ë ¨ ìž…ìž¥ ë°œí‘œ - [ëê¹Œì§€LIVE] MBC ì¤‘ê³„ë°©ì†¡ 2024ë…„ 12ì›” 14ì¼ | url: https://www.youtube.com/watch?v=gLlN9BN_2X0&pp=ygUY7ZWc6rWtIOuMgO2GteuguSDtg4TtlbUg\n",
      "Similarity: 83.15 | Title: [ì—ë””í„°í”½] íƒ„í•µì•ˆ ê°€ê²°ì—â€¦'í„°ì¤ëŒ€ê°' í™ì¤€í‘œê°€ ë‚¨ê¸´ ë§ / YTN | url: https://www.youtube.com/watch?v=cDbU7e2ywLw&pp=ygUY7ZWc6rWtIOuMgO2GteuguSDtg4TtlbUg\n"
     ]
    }
   ],
   "source": [
    "for num in new_list:\n",
    "    title = similarity_key_dict[str(num)][\"title\"]\n",
    "    url = similarity_key_dict[str(num)][\"url\"]\n",
    "    print(f\"Similarity: {num} | Title: {title} | url: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[1;32m----> 6\u001b[0m nli_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroberta-large-mnli\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m headline \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWildfire Spreads Rapidly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to strong winds, the wildfire has spread faster than expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jhnam\\anaconda3\\envs\\langchain\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\jhnam\\anaconda3\\envs\\langchain\\Lib\\site-packages\\transformers\\pipelines\\base.py:240\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    246\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp_pipline = pipeline(\"device=\")\n",
    "\n",
    "nli_pipeline = pipeline(\"text-classification\", model=\"roberta-large-mnli\")\n",
    "headline = \"Wildfire Spreads Rapidly\"\n",
    "text = \"Due to strong winds, the wildfire has spread faster than expected.\"\n",
    "\n",
    "result = nli_pipeline(f\"{headline} [SEP] {text}\")\n",
    "print(result)  # {'label': 'ENTAILMENT', 'score': 0.85}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
